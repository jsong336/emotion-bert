{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tweet Emotions Analysis (12 emotions) <a class=\"anchor\" id=\"tea\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://www.linkedin.com/in/ouassim-adnane/\">Ouassim Adnane</a> 08 June 2020","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://www.feelingfacescards.com/images/feeling_faces_chart_poster.jpg\" />","metadata":{}},{"cell_type":"markdown","source":"# Overview ","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I've used a tweets dataset that contains tweet text with 12 emotions (neutral, worry, happiness, sadness, love, surprise, fun, relief, hate, empty, enthusiasm, boredom and anger) and the goal is to predict the percentage of emotions in a giving text","metadata":{}},{"cell_type":"markdown","source":"To achieve that goal I've used some techniques fist to preprocess the text data :\n\n<li>correct misspelled text</li>\n<li>replace English contractions with there meaning (isn't => is not)</li>\n<li>remove some punctuations, URLS user mentions and extra spaces</li>\n<li>replace emojis with there meaning</li><br>\n\nFor the modeling part I've used LSTM's and Roberta base Model:\n<li>First a Basic LSTM </li>\n<li>LSTM model with glove word embeddings</li>\n<li>Roberta Base model </li>\n<br>\nIn the final part, I've made a donut chart that detects the level of emotions is a particular text.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color:red\">If you enjoyed this work or you found it helpful , an upvotes would be very much appreciated  :-)</h2>","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<h3>Contents</h3>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#tu\">Tutorials</a></li>\n<li style=\"list-style: outside none none !important;\"><a href=\"#dp\">1 Data processing</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#dp-md\">1.1 Misspelled data</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#dp-c\">1.2 Contractions</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#dp-r\">1.3 Remove URLS and mentions</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#dp-p\">1.4 Punctuations and emojis</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#dp-re\">1.5 Remove empty comments</a></li>\n      </ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"#m\">2 Modeling and Result</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-ed\">2.1 Encoding the data and train test split</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-l\">2.2 LSTM</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-lr\">2.3 LSTM Results</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-lg\">2.4 LSTM with Glove</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-lgr\">2.5 LSTM with Glove Results</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-rb\">2.6 Roberta Base Model</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#m-rbr\">2.7 Roberta Base Model Results</a></li>\n      </ul>\n\n</ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Tutorials <a class=\"anchor\" id=\"tu\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"markdown","source":"List of some blog post for all the NLP techniques used in this notebook","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <ul style=\"  list-style-type: none;width: 800px;\">\n    <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" style=\"text-decoration:none;color:black\" target=\"_blank\">\n    <li style=\"float: left;margin: 0 15px 0 0;font: 200 12px/1.5 Georgia, Times New Roman, serif;padding: 5px;overflow: auto;\">\n        <img src=\"https://colah.github.io/images/post-covers/lstm.png\" style=\"float: left;margin: 0 15px 0 0;width:200px;hight:300px\">\n      <p style=\"font: bold 20px/1.5 Helvetica, Verdana, sans-serif;\">Understanding LSTM Networks</p>\n      <p style=\"font: 200 12px/1.5 Georgia, Times New Roman, serif;\">Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence....</p>\n    </li>\n      </a>\n      <hr style=\"width:100%;text-align:left;margin-left:0\">  \n    <a href=\"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\" style=\"text-decoration:none;color:black\" target=\"_blank\">\n    <li style=\"float: left;margin: 0 15px 0 0;font: 200 12px/1.5 Georgia, Times New Roman, serif;padding: 5px;overflow: auto;\">\n        <img src=\"https://miro.medium.com/max/2456/1*gcC7b_v7OKWutYN1NAHyMQ.png\" style=\"float: left;margin: 0px 15px 0 0;width:200px;hight:300px\">\n      <p style=\"font: bold 20px/1.5 Helvetica, Verdana, sans-serif;\"> Understanding of Word Embeddings</p>\n      <p style=\"font: 200 12px/1.5 Georgia, Times New Roman, serif;\">In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked – Why do we need Word Embeddings?...\n</p>\n    </li>\n      </a>\n      <hr style=\"width:100%;text-align:left;margin-left:0\">  \n    <a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\" style=\"text-decoration:none;color:black\" target=\"_blank\">\n    <li style=\"float: left;margin: 0 15px 0 0;font: 200 12px/1.5 Georgia, Times New Roman, serif;padding: 5px;overflow: auto;\">\n        <img src=\"https://www.marcaglobal.us/wp-content/uploads/2019/11/BERT-2.png\" style=\"float: left;margin: 0px 15px 0 0;width:200px;hight:300px\">\n      <p style=\"font: bold 20px/1.5 Helvetica, Verdana, sans-serif;\">A Visual Guide to Using BERT for the First Time</p>\n      <p style=\"font: 200 12px/1.5 Georgia, Times New Roman, serif;\">This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.....\n</p>\n    </li>\n      </a>\n      <hr style=\"width:100%;text-align:left;margin-left:0\">  \n    <a href=\"https://huggingface.co/transformers/model_doc/roberta.html\" style=\"text-decoration:none;color:black\" target=\"_blank\">\n    <li style=\"float: left;margin: 0 15px 0 0;font: 200 12px/1.5 Georgia, Times New Roman, serif;padding: 5px;overflow: auto;\">\n        <img src=\"https://cdn-images-1.medium.com/max/1000/0*TWEo06DiQYlP5XSi\" style=\"float: left;margin: 0px 15px 0 0;width:200px;hight:300px\">\n      <p style=\"font: bold 20px/1.5 Helvetica, Verdana, sans-serif;\">RoBERTa</p>\n      <p style=\"font: 200 12px/1.5 Georgia, Times New Roman, serif;\">The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018....</p>\n    </li>\n      </a>\n\n  </ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"!pip install tweet-preprocessor 2>/dev/null 1>/dev/null","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:00.058181Z","iopub.execute_input":"2022-03-15T22:47:00.05851Z","iopub.status.idle":"2022-03-15T22:47:10.575163Z","shell.execute_reply.started":"2022-03-15T22:47:00.058479Z","shell.execute_reply":"2022-03-15T22:47:10.573946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import preprocessor as p\nimport numpy as np \nimport pandas as pd \nimport emoji\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom tqdm import tqdm","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T22:47:10.579588Z","iopub.execute_input":"2022-03-15T22:47:10.579957Z","iopub.status.idle":"2022-03-15T22:47:19.239185Z","shell.execute_reply.started":"2022-03-15T22:47:10.579923Z","shell.execute_reply":"2022-03-15T22:47:19.236618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation  <a class=\"anchor\" id=\"dp\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"#data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\n#data2 = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/figure-eight-labelled-textual-dataset/text_emotion.csv\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-03-15T22:47:24.522672Z","iopub.execute_input":"2022-03-15T22:47:24.523119Z","iopub.status.idle":"2022-03-15T22:47:24.661872Z","shell.execute_reply.started":"2022-03-15T22:47:24.523084Z","shell.execute_reply":"2022-03-15T22:47:24.660902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misspelled data <a class=\"anchor\" id=\"dp-md\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"misspell_data = pd.read_csv(\"/kaggle/input/spelling/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\nmisspell_data.misspell = misspell_data.misspell.str.strip()\nmisspell_data.misspell = misspell_data.misspell.str.split(\" \")\nmisspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\nmisspell_data.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n\n#Sample of the dict\n{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:27.729231Z","iopub.execute_input":"2022-03-15T22:47:27.729692Z","iopub.status.idle":"2022-03-15T22:47:27.779351Z","shell.execute_reply.started":"2022-03-15T22:47:27.729626Z","shell.execute_reply":"2022-03-15T22:47:27.778194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def misspelled_correction(val):\n    for x in val.split(): \n        if x in miss_corr.keys(): \n            val = val.replace(x, miss_corr[x]) \n    return val\n\ndata[\"clean_content\"] = data.content.apply(lambda x : misspelled_correction(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:28.457933Z","iopub.execute_input":"2022-03-15T22:47:28.458343Z","iopub.status.idle":"2022-03-15T22:47:28.614159Z","shell.execute_reply.started":"2022-03-15T22:47:28.458295Z","shell.execute_reply":"2022-03-15T22:47:28.613185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Contractions <a class=\"anchor\" id=\"dp-c\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"contractions = pd.read_csv(\"/kaggle/input/contractions/contractions.csv\")\ncont_dic = dict(zip(contractions.Contraction, contractions.Meaning))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:29.758951Z","iopub.execute_input":"2022-03-15T22:47:29.759432Z","iopub.status.idle":"2022-03-15T22:47:29.77308Z","shell.execute_reply.started":"2022-03-15T22:47:29.759399Z","shell.execute_reply":"2022-03-15T22:47:29.771993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cont_to_meaning(val): \n  \n    for x in val.split(): \n        if x in cont_dic.keys(): \n            val = val.replace(x, cont_dic[x]) \n    return val\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:31.489882Z","iopub.execute_input":"2022-03-15T22:47:31.490349Z","iopub.status.idle":"2022-03-15T22:47:31.496905Z","shell.execute_reply.started":"2022-03-15T22:47:31.490311Z","shell.execute_reply":"2022-03-15T22:47:31.495691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.clean_content = data.clean_content.apply(lambda x : cont_to_meaning(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:31.895127Z","iopub.execute_input":"2022-03-15T22:47:31.8955Z","iopub.status.idle":"2022-03-15T22:47:32.057312Z","shell.execute_reply.started":"2022-03-15T22:47:31.895466Z","shell.execute_reply":"2022-03-15T22:47:32.056238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove URLS and mentions <a class=\"anchor\" id=\"dp-r\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"p.set_options(p.OPT.MENTION, p.OPT.URL)\np.clean(\"hello guys @alx #sport🔥 1245 https://github.com/s/preprocessor\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:33.953271Z","iopub.execute_input":"2022-03-15T22:47:33.953675Z","iopub.status.idle":"2022-03-15T22:47:33.961682Z","shell.execute_reply.started":"2022-03-15T22:47:33.953643Z","shell.execute_reply":"2022-03-15T22:47:33.960474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"clean_content\"]=data.content.apply(lambda x : p.clean(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T22:47:34.137162Z","iopub.execute_input":"2022-03-15T22:47:34.137567Z","iopub.status.idle":"2022-03-15T22:47:37.352053Z","shell.execute_reply.started":"2022-03-15T22:47:34.137534Z","shell.execute_reply":"2022-03-15T22:47:37.350923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Punctuations and emojis <a class=\"anchor\" id=\"dp-p\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"def punctuation(val): \n  \n    punctuations = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n  \n    for x in val.lower(): \n        if x in punctuations: \n            val = val.replace(x, \" \") \n    return val\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuation(\"test @ #ldfldlf??? !! \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.clean_content = data.clean_content.apply(lambda x : ' '.join(punctuation(emoji.demojize(x)).split()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(val):\n    val = misspelled_correction(val)\n    val = cont_to_meaning(val)\n    val = p.clean(val)\n    val = ' '.join(punctuation(emoji.demojize(val)).split())\n    \n    return val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text(\"isn't 💡 adultry @ttt good bad ... ! ? \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove empty comments <a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"data = data[data.clean_content != \"\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling  <a class=\"anchor\" id=\"m\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"markdown","source":"### Encoding the data and train test split <a class=\"anchor\" id=\"m-ed\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"sent_to_id  = {\"empty\":0, \"sadness\":1,\"enthusiasm\":2,\"neutral\":3,\"worry\":4,\n                        \"surprise\":5,\"love\":6,\"fun\":7,\"hate\":8,\"happiness\":9,\"boredom\":10,\"relief\":11,\"anger\":12}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"sentiment_id\"] = data['sentiment'].map(sent_to_id)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(data.sentiment_id)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nY = onehot_encoder.fit_transform(integer_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM <a class=\"anchor\" id=\"m-l\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 160\nEpoch = 5\ntoken.fit_on_texts(list(X_train) + list(X_test))\nX_train_pad = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_len)\nX_test_pad = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_idx = token.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 160\nlstm_out = 250\n\nmodel = Sequential()\nmodel.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(keras.layers.core.Dense(13, activation='softmax'))\n#adam rmsprop \nmodel.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentiment(model,text):\n    text = clean_text(text)\n    #tokenize\n    twt = token.texts_to_sequences([text])\n    twt = sequence.pad_sequences(twt, maxlen=max_len, dtype='int32')\n    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n    result.columns = [\"sentiment\",\"percentage\"]\n    result=result[result.percentage !=0]\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_result(df):\n    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])\n    #fig.show()\n    colors={'love':'rgb(213,0,0)','empty':'rgb(0,0,0)',\n                    'sadness':'rgb(0,142,248)','enthusiasm':'rgb(245,178,123)',\n                    'neutral':'rgb(237,236,236)','worry':'rgb(216,74,9)',\n                    'surprise':'rgb(1,155,189)','fun':'rgb(255,208,0)',\n                    'hate':'rgb(120,0,160)','happiness':'rgb(9,143,69)',\n                    'boredom':'rgb(128,124,124)','relief':'rgb(133,221,233)',\n                    'anger':'rgb(245,94,16)'}\n    col_2={}\n    for i in result.sentiment.to_list():\n        col_2[i]=colors[i]\n    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test LSTM Results <a class=\"anchor\" id=\"m-lr\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"result =get_sentiment(model,\"Had an absolutely brilliant day ðŸ˜ loved seeing an old friend and reminiscing\")\nplot_result(result)\nresult =get_sentiment(model,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can’t hold myself back. I really miss you\")\nplot_result(result)\nresult =get_sentiment(model,\"I hate this game so much,It make me angry all the time \")\nplot_result(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM with glove 6B 200d word embedding <a class=\"anchor\" id=\"m-lg\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"def read_data(file_name):\n    with open(file_name,'r') as f:\n        word_vocab = set() \n        word2vector = {}\n        for line in f:\n            line_ = line.strip() \n            words_Vec = line_.split()\n            word_vocab.add(words_Vec[0])\n            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab, word_to_idx =read_data(\"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(w_idx) + 1, 200))\nfor word, i in w_idx.items():\n    embedding_vector = word_to_idx.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 200\nlstm_out = 250\n\nmodel_lstm_gwe = Sequential()\nmodel_lstm_gwe.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1],weights=[embedding_matrix],trainable=False))\nmodel_lstm_gwe.add(SpatialDropout1D(0.2))\nmodel_lstm_gwe.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel_lstm_gwe.add(keras.layers.core.Dense(13, activation='softmax'))\n#adam rmsprop \nmodel_lstm_gwe.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\nprint(model_lstm_gwe.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_lstm_gwe.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test LSTM glove Results <a class=\"anchor\" id=\"m-lgr\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"result =get_sentiment(model_lstm_gwe,\"Had an absolutely brilliant day ðŸ˜ loved seeing an old friend and reminiscing\")\nplot_result(result)\nresult =get_sentiment(model_lstm_gwe,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can’t hold myself back. I really miss you\")\nplot_result(result)\nresult =get_sentiment(model_lstm_gwe,\"I hate this game so much,It make me angry all the time \")\nplot_result(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Roberta Base Model <a class=\"anchor\" id=\"m-rb\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n\ndef build_model(transformer, max_len=160):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(13, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nMODEL = 'roberta-base'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)\nX_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_test_t, y_test))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_layer = TFAutoModel.from_pretrained(MODEL)\nmodel_roberta_base = build_model(transformer_layer, max_len=max_len)\nmodel_roberta_base.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = X_train.shape[0] // batch_size\nmodel_roberta_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Roberta Model Results <a class=\"anchor\" id=\"m-rbr\"></a>\n<a href=\"#toc\"><img src= \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Circle-icons-arrow-up.svg/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents</a>","metadata":{}},{"cell_type":"code","source":"def get_sentiment2(model,text):\n    text = clean_text(text)\n    #tokenize\n    x_test1 = regular_encode([text], tokenizer, maxlen=max_len)\n    test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))\n    #test1\n    sentiment = model.predict(test1,verbose = 0)\n    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n    result.columns = [\"sentiment\",\"percentage\"]\n    result=result[result.percentage !=0]\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result =get_sentiment2(model_roberta_base,\"Had an absolutely brilliant day ðŸ˜ loved seeing an old friend and reminiscing\")\nplot_result(result)\nresult =get_sentiment2(model_roberta_base,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can’t hold myself back. I really miss you\")\nplot_result(result)\nresult =get_sentiment2(model_roberta_base,\"I hate this game so much,It make me angry all the time \")\nplot_result(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Albert Base","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nMODEL = 'albert-base-v2'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nX_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)\nX_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_test_t, y_test))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_layer = TFAutoModel.from_pretrained(MODEL)\nalbert = build_model(transformer_layer, max_len=max_len)\nalbert.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = X_train.shape[0] // batch_size\nalbert.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result =get_sentiment2(albert,\"Had an absolutely brilliant day ðŸ˜ loved seeing an old friend and reminiscing\")\nplot_result(result)\nresult =get_sentiment2(albert,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can’t hold myself back. I really miss you\")\nplot_result(result)\nresult =get_sentiment2(albert,\"I hate this game so much,It make me angry all the time \")\nplot_result(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:red\">If you enjoyed this work or you found it helpful , an upvotes would be very much appreciated  :-)</h2>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#tea\"><img  src=\"https://za.heytv.org/wp-content/uploads/2019/08/AGF-l79DYZtk_pSyfWgIP3D-3yi8YN6ZeWO0E8tyLgs800-c-k-c0xffffffff-no-rj-mo.jpeg\" style=\"height: 300px\"/></a>","metadata":{}}]}