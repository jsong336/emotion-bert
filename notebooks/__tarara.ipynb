{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lDBZcPC1yiw6"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import re\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd\n",
        "import preprocessor\n",
        "import torch \n",
        "import shutil\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModel , \n",
        "    AdamW, get_linear_schedule_with_warmup)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from torch import Tensor \n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from attrdict import AttrDict\n",
        "\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "tqdm.pandas()\n",
        "\n",
        "input_dir = '../inputs'\n",
        "output_dir = '../outputs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kI1iwpH70ELN"
      },
      "outputs": [],
      "source": [
        "args = AttrDict({\n",
        "    'model_name': 'go-emotion-tiny',\n",
        "    'encoder_name': 'prajjwal1/bert-tiny',\n",
        "    'encoder_dim': 128, \n",
        "    'fc_hiddens': [50, 50], \n",
        "    'dropout_p': 0.1, \n",
        "    'sentence_max_len': 50,\n",
        "    'train_epochs': 5, \n",
        "    'train_batch_size': 32, \n",
        "    'eval_batch_size': 32, \n",
        "    'learning_rate': 0.0001, \n",
        "    'grad_clip_max': 1.0,\n",
        "    'weight_decay': 1e-4, \n",
        "    'warmup_ratio': 1e-1, \n",
        "    'classification_threshold': 0.5,\n",
        "    'validation_split_from_train': 0.2, \n",
        "    'test_split': 0.2,\n",
        "    'validation_steps': 3e4, \n",
        "    'save_steps': 3e4, \n",
        "    'dataset_source_path': os.path.join(input_dir, 'go-emotions-google-emotions-dataset', 'go_emotions_dataset.csv'), \n",
        "    'checkpoint_dir': os.path.join(output_dir, 'checkpoints'),\n",
        "    'train_dataset_path': os.path.join(output_dir, 'train_set_compact.csv'), \n",
        "    'test_dataset_path': os.path.join(output_dir, 'test_set_compact.csv'),\n",
        "    'seed': 0, \n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu', \n",
        "    'drop_insignifiant': True, \n",
        "    'emotions': ['admiration', 'amusement', 'anger', 'annoyance', 'approval', \n",
        "                 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', \n",
        "                 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', \n",
        "                 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', \n",
        "                 'pride', 'realization', 'relief', 'remorse', 'sadness', \n",
        "                 'surprise'], # 'neutral' \n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "74diy0VT6gLJ"
      },
      "outputs": [],
      "source": [
        "def init_seed(args):\n",
        "  torch.manual_seed(args.seed)\n",
        "  random.seed(args.seed)\n",
        "  np.random.seed(args.seed)\n",
        "  return\n",
        "\n",
        "init_seed(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHbNh2hn_JpD",
        "outputId": "def7d4bd-1a00-48c6-f71d-2dac51daff8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(args.encoder_name)\n",
        "encoder = AutoModel.from_pretrained(args.encoder_name)\n",
        "\n",
        "def tokenizing_input(texts, tokenizer, maxlen=50):\n",
        "    result = tokenizer.batch_encode_plus(\n",
        "            texts, \n",
        "          return_attention_mask=True, \n",
        "        return_token_type_ids=False,\n",
        "        padding='longest', \n",
        "        max_length=maxlen)\n",
        "    return result['input_ids'], result['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i5C-tDBL-xxf"
      },
      "outputs": [],
      "source": [
        "class ZipDataset(Dataset):\n",
        "  def __init__(self, datasets):\n",
        "    super(ZipDataset, self).__init__()\n",
        "    self.keys = list(datasets.keys())\n",
        "    self.values = list(datasets.values()) \n",
        "    self.datasets = datasets\n",
        "    assert all([len(self.values[0]) == len(v) for v in self.values])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.values[0])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    item = {}\n",
        "    for k, v in self.datasets.items():\n",
        "      item[k] = v[idx]\n",
        "    return item\n",
        "\n",
        "def generate_dataset(args, tokenizer):\n",
        "  Dtr = pd.read_csv(args.train_dataset_path)\n",
        "  X = Dtr['text'].to_numpy()\n",
        "  y = Dtr[list(args.emotions)].to_numpy()\n",
        "\n",
        "  Xtr, Xval, ytr, yval = train_test_split(X, y, \n",
        "                                          test_size=args.validation_split_from_train, \n",
        "                                          shuffle=False)\n",
        "\n",
        "  Xtr_tk, Xtr_mask= tokenizing_input(Xtr.tolist(), tokenizer, \n",
        "                                     maxlen=args.sentence_max_len)\n",
        "  Xval_tk, Xval_mask = tokenizing_input(Xval.tolist(), tokenizer, \n",
        "                                        maxlen=args.sentence_max_len)\n",
        "\n",
        "  train_dataset = ZipDataset({\n",
        "      'input_ids': Tensor(Xtr_tk).type(torch.int32), \n",
        "      'attention_mask': Tensor(Xtr_mask).type(torch.int32), \n",
        "      'y_true': Tensor(ytr).type(torch.float32)\n",
        "  })\n",
        "\n",
        "  val_dataset = ZipDataset({\n",
        "      'input_ids': Tensor(Xval_tk).type(torch.int32), \n",
        "      'attention_mask': Tensor(Xval_mask).type(torch.int32), \n",
        "      'y_true': Tensor(yval).type(torch.float32)\n",
        "  })\n",
        "\n",
        "  return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_WSKqmgzc1M",
        "outputId": "0e18d8fc-68f2-4d5b-b79e-0d9e6ad9f50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 8.57 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "train_dataset, val_dataset = generate_dataset(args, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hMmjh5xS_xG9"
      },
      "outputs": [],
      "source": [
        "def compute_classification_metrics(y_true, proba, threshold):\n",
        "  assert len(y_true) == len(proba), 'y_true and y_pred length mismatch {} {}'.format(len(preds), len(labels))\n",
        "\n",
        "  results = {}\n",
        "  y_true = y_true.astype(int)\n",
        "  y_pred = (proba >= threshold).astype(int)\n",
        "\n",
        "  results[\"accuracy\"] = (y_true == y_pred).mean()\n",
        "  if (np.unique(y_true) == 1).sum() == 0:\n",
        "    results[\"auc_roc_macro\"] = roc_auc_score(y_true, proba, average='macro')\n",
        "    results[\"auc_roc_micro\"] = roc_auc_score(y_true, proba, average='micro')\n",
        "  results[\"macro_precision\"], results[\"macro_recall\"], results[\"macro_f1\"], _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
        "  results[\"micro_precision\"], results[\"micro_recall\"], results[\"micro_f1\"], _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
        "  results[\"weighted_precision\"], results[\"weighted_recall\"], results[\"weighted_f1\"], _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AA5TgjDEMkgl"
      },
      "outputs": [],
      "source": [
        "class GoEmotionClassiferWithGRU(torch.nn.Module):\n",
        "  def __init__(self, \n",
        "               encoder,\n",
        "               encoder_dim=128, \n",
        "               seq_len=82,\n",
        "               rnn_hidden = 50,  \n",
        "               rnn_num_layers = 1,\n",
        "               bidirectional=True, \n",
        "               hiddens = None,\n",
        "               dropout_p=0.1, \n",
        "               n_cls:int = 28, \n",
        "               criterion = torch.nn.BCEWithLogitsLoss()\n",
        "               ):\n",
        "    super(GoEmotionClassiferWithGRU, self).__init__()\n",
        "\n",
        "    self.encoder_dim = encoder_dim\n",
        "    self.rnn_hidden = rnn_hidden\n",
        "    self.rnn_num_layers = rnn_num_layers\n",
        "    self.seq_len = seq_len\n",
        "    self.bidirectional = bidirectional\n",
        "    self.hiddens = [100] if hiddens is None else hiddens \n",
        "    self.dropout_p = dropout_p \n",
        "    self.n_cls = n_cls\n",
        "    self.criterion = criterion\n",
        "\n",
        "    # layers\n",
        "    self.encoder = encoder\n",
        "    self.gru = torch.nn.GRU(\n",
        "        input_size= encoder_dim, \n",
        "        hidden_size = rnn_hidden, \n",
        "        batch_first = True, \n",
        "        bidirectional = bidirectional\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "    # full connected\n",
        "    fcs = []\n",
        "    in_feature = (int(bidirectional) + 1) * rnn_hidden\n",
        "    for h in self.hiddens:\n",
        "      fcs.append(torch.nn.Linear(in_feature, h))\n",
        "      fcs.append(torch.nn.ReLU())\n",
        "      in_feature = h    \n",
        "    fcs.append(torch.nn.Linear(in_feature, n_cls))\n",
        "    self.fcs = torch.nn.Sequential(*fcs)\n",
        "\n",
        "  def forward(self, X_tk, X_mask, y_true=None):\n",
        "    encoder_output = self.encoder(X_tk, X_mask)\n",
        "    contextual_emb = encoder_output['last_hidden_state'] # contextual embedding\n",
        "    output, _ = self.gru(contextual_emb)\n",
        "    output = output[:, -1, :]\n",
        "    z = self.dropout(output) \n",
        "    logits = self.fcs(z)\n",
        "\n",
        "    if not (y_true is None):\n",
        "      loss = self.criterion(logits, y_true)\n",
        "      return (loss, logits)\n",
        "    return logits\n",
        "\n",
        "  def save_pretrained(self, path):\n",
        "    encoder_path = os.path.join(path, 'encoder')\n",
        "    pt_path = os.path.join(path, 'model.pt')\n",
        "    encoder = self.encoder\n",
        "    encoder.save_pretrained(encoder_path)\n",
        "    self.encoder = None\n",
        "    torch.save({\n",
        "        'model': self.state_dict(), \n",
        "        'config': {\n",
        "            'encoder': encoder.config.to_dict(), \n",
        "            'criterion': self.criterion.__class__.__name__, \n",
        "            'architecture': str(self), \n",
        "            'rnn_hidden': self.rnn_hidden,\n",
        "            'rnn_num_layers': self.rnn_num_layers,\n",
        "            'seq_len': self.seq_len,\n",
        "            'bidirectional': self.bidirectional,\n",
        "            'hiddens': self.hiddens, \n",
        "            'dropout_p': self.dropout_p, \n",
        "            'n_cls': self.n_cls\n",
        "        }\n",
        "    }, pt_path)\n",
        "    self.encoder = encoder\n",
        "    return\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, path):\n",
        "    pt_path = os.path.join(path, 'model.pt')\n",
        "    encoder_path = os.path.join(path, 'encoder')\n",
        "    saved_model = torch.load(pt_path, map_location=torch.device('cpu'))\n",
        "    encoder = AutoModel.from_pretrained(encoder_path)\n",
        "    model = cls(\n",
        "        encoder=None,\n",
        "        encoder_dim = saved_model['config']['encoder']['hidden_size'], \n",
        "        seq_len = saved_model['config']['seq_len'],\n",
        "        rnn_hidden = saved_model['config']['rnn_hidden'],  \n",
        "        rnn_num_layers = saved_model['config']['rnn_num_layers'],\n",
        "        bidirectional= saved_model['config']['bidirectional'], \n",
        "        hiddens = saved_model['config']['hiddens'],\n",
        "        dropout_p = saved_model['config']['dropout_p'],\n",
        "        n_cls = saved_model['config']['n_cls'],\n",
        "        criterion = getattr(torch.nn, saved_model['config']['criterion'])()\n",
        "    )\n",
        "    model.load_state_dict(saved_model['model'])\n",
        "    model.encoder = encoder\n",
        "    return model \n",
        "\n",
        "class GoEmotionClassifer(torch.nn.Module):\n",
        "  def __init__(self, \n",
        "               encoder,\n",
        "               encoder_dim=128, \n",
        "               hiddens = None,\n",
        "               dropout_p=0.1, \n",
        "               n_cls:int = 28, \n",
        "               criterion = torch.nn.BCEWithLogitsLoss()\n",
        "               ):\n",
        "    super(GoEmotionClassifer, self).__init__()\n",
        "\n",
        "    self.encoder_dim = encoder_dim\n",
        "    self.hiddens = [100] if hiddens is None else hiddens \n",
        "    self.dropout_p = dropout_p \n",
        "    self.n_cls = n_cls\n",
        "    self.criterion = criterion\n",
        "\n",
        "    # layers\n",
        "    self.encoder = encoder\n",
        "    self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "    # full connected\n",
        "    fcs = []\n",
        "    in_feature = encoder_dim \n",
        "    for h in self.hiddens:\n",
        "      fcs.append(torch.nn.Linear(in_feature, h))\n",
        "      fcs.append(torch.nn.ReLU())\n",
        "      in_feature = h    \n",
        "    fcs.append(torch.nn.Linear(in_feature, n_cls))\n",
        "    self.fcs = torch.nn.Sequential(*fcs)\n",
        "\n",
        "  def forward(self, X_tk, X_mask, y_true=None):\n",
        "    encoder_output = self.encoder(X_tk, X_mask) # contextual embedding\n",
        "    pooled_output = encoder_output['pooler_output'] \n",
        "    z = self.dropout(pooled_output) \n",
        "    logits = self.fcs(z)\n",
        "\n",
        "    if not (y_true is None):\n",
        "      loss = self.criterion(logits, y_true)\n",
        "      return (loss, logits)\n",
        "    return logits\n",
        "\n",
        "  def save_pretrained(self, path):\n",
        "    encoder_path = os.path.join(path, 'encoder')\n",
        "    pt_path = os.path.join(path, 'model.pt')\n",
        "    encoder = self.encoder\n",
        "    encoder.save_pretrained(encoder_path)\n",
        "    self.encoder = None\n",
        "    torch.save({\n",
        "        'model': self.state_dict(), \n",
        "        'config': {\n",
        "            'encoder': encoder.config.to_dict(), \n",
        "            'criterion': self.criterion.__class__.__name__, \n",
        "            'architecture': str(self), \n",
        "            'hiddens': self.hiddens, \n",
        "            'dropout_p': self.dropout_p, \n",
        "            'n_cls': self.n_cls\n",
        "        }\n",
        "    }, pt_path)\n",
        "    self.encoder = encoder\n",
        "    return\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, path):\n",
        "    pt_path = os.path.join(path, 'model.pt')\n",
        "    encoder_path = os.path.join(path, 'encoder')\n",
        "    saved_model = torch.load(pt_path, map_location=torch.device('cpu'))\n",
        "    encoder = AutoModel.from_pretrained(encoder_path)\n",
        "    model = cls(\n",
        "        encoder=None,\n",
        "        encoder_dim = saved_model['config']['encoder']['hidden_size'], \n",
        "        hiddens = saved_model['config']['hiddens'],\n",
        "        dropout_p = saved_model['config']['dropout_p'],\n",
        "        n_cls = saved_model['config']['n_cls'],\n",
        "        criterion = getattr(torch.nn, saved_model['config']['criterion'])()\n",
        "    )\n",
        "    model.load_state_dict(saved_model['model'])\n",
        "    model.encoder = encoder\n",
        "    return model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PmYLFsL929A4"
      },
      "outputs": [],
      "source": [
        "def proba_on_examples(X, model, tokenizer=None):\n",
        "  if tokenizer is not None:\n",
        "    X_tk, X_mask = tokenizing_input(X, tokenizer)\n",
        "    X_tk = torch.Tensor(X_tk).type(torch.int32)\n",
        "    X_mask = torch.Tensor(X_mask).type(torch.int32)\n",
        "  else:\n",
        "    X_tk, X_mask = X['input_ids'], X['attention_mask']\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(X_tk, X_mask).numpy()\n",
        "    proba = 1 / (1 + np.exp(-logits))\n",
        "\n",
        "  return proba\n",
        "\n",
        "def proba_to_emotion(proba, threshold, emotions):\n",
        "  assert proba.shape[-1] == len(emotions), 'emotions and proba mismatch {} vs {}'.format(len(emotions), proba.shape[-1])\n",
        "  emotions = np.array(emotions)\n",
        "  return [tuple(emotions[p >= threshold]) for p in proba]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsm5Poq_D0Z2",
        "outputId": "4cfe2a88-f73e-4b5c-d6a7-2d99b4eebb38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('admiration',\n",
              "  'annoyance',\n",
              "  'caring',\n",
              "  'confusion',\n",
              "  'curiosity',\n",
              "  'desire',\n",
              "  'excitement',\n",
              "  'gratitude',\n",
              "  'nervousness',\n",
              "  'optimism',\n",
              "  'realization',\n",
              "  'relief',\n",
              "  'remorse'),\n",
              " ('admiration',\n",
              "  'annoyance',\n",
              "  'caring',\n",
              "  'confusion',\n",
              "  'curiosity',\n",
              "  'desire',\n",
              "  'excitement',\n",
              "  'gratitude',\n",
              "  'nervousness',\n",
              "  'optimism',\n",
              "  'pride',\n",
              "  'relief',\n",
              "  'remorse')]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_model_from_config(args, encoder):\n",
        "  return GoEmotionClassifer(\n",
        "    encoder, \n",
        "    encoder_dim = args.encoder_dim, \n",
        "    hiddens = args.fc_hiddens, \n",
        "    dropout_p = args.dropout_p, \n",
        "    n_cls = len(args.emotions)\n",
        "  )\n",
        "\n",
        "def create_rnn_model_from_config(args, encoder):\n",
        "  return GoEmotionClassiferWithGRU(\n",
        "      encoder, \n",
        "      encoder_dim = args.encoder_dim, \n",
        "      seq_len = args.seq_len, \n",
        "      rnn_hidden = args.rnn_hidden, \n",
        "      rnn_num_layers = args.rnn_num_layers,\n",
        "      bidirectional = args.bidirectional, \n",
        "      hiddens = args.fc_hiddens, \n",
        "      dropout_p = args.dropout_p, \n",
        "      n_cls = len(args.emotions)\n",
        "  )\n",
        "\n",
        "test_args = args.copy()\n",
        "test_args['seq_len'] = 82\n",
        "test_args['rnn_hidden'] = 50\n",
        "test_args['rnn_num_layers'] = 1\n",
        "test_args['bidirectional'] = True\n",
        "test_args['hiddens'] = [50, 50]\n",
        "test_args = AttrDict(test_args)\n",
        "# model = create_model_from_config(args, encoder)\n",
        "model = create_rnn_model_from_config(test_args, encoder)\n",
        "proba = proba_on_examples(['hello my name is jeongwon', 'its me jeongwon'],  model, tokenizer)\n",
        "proba_to_emotion(proba, args.classification_threshold, emotions=args.emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPJa_quWJ0DI",
        "outputId": "a1fd619c-64e1-4436-b29d-cbfaf3c9114b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.044444444444444446,\n",
              " 'macro_precision': 0.044444444444444446,\n",
              " 'macro_recall': 0.14814814814814814,\n",
              " 'macro_f1': 0.0670194003527337,\n",
              " 'micro_precision': 0.044444444444444446,\n",
              " 'micro_recall': 1.0,\n",
              " 'micro_f1': 0.0851063829787234,\n",
              " 'weighted_precision': 0.3333333333333333,\n",
              " 'weighted_recall': 1.0,\n",
              " 'weighted_f1': 0.49206349206349215}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "proba = proba_on_examples(val_dataset[:5], model)\n",
        "y_true = val_dataset[:5]['y_true'].numpy()\n",
        "compute_classification_metrics(y_true, proba, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sfIG2HZH1X37"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, \n",
        "             dataset, \n",
        "             batch_size=16, \n",
        "             threshold=0.5,\n",
        "             device='cpu', back_to_cpu=True):\n",
        "  eval_dataloader = DataLoader(\n",
        "      dataset, \n",
        "      batch_size=batch_size, \n",
        "  )\n",
        "\n",
        "  n_batch = 0\n",
        "  total_loss = 0.0\n",
        "  y_true = []\n",
        "  proba = []\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  for batch in tqdm(eval_dataloader, desc='evaluation', leave=False):\n",
        "    model.eval()\n",
        "    batch = { k:v.to(device) for k, v in batch.items() }\n",
        "\n",
        "    with torch.no_grad():\n",
        "      loss_per_batch, logits = model(batch['input_ids'], \n",
        "                               batch['attention_mask'], \n",
        "                               batch['y_true'])\n",
        "      total_loss += loss_per_batch.item()\n",
        "\n",
        "      logits = logits.cpu().detach().numpy()\n",
        "\n",
        "    p = 1 / (1 + np.exp(-logits))\n",
        "    proba.append(p)\n",
        "    y_true.append(batch['y_true'].cpu().detach().numpy())\n",
        "\n",
        "    n_batch += 1\n",
        "\n",
        "  if back_to_cpu:\n",
        "    model.cpu()\n",
        "\n",
        "  proba = np.vstack(proba)\n",
        "  y_true = np.vstack(y_true)\n",
        "  results = {\n",
        "      'loss': total_loss / n_batch, \n",
        "      'trigger_rate': (proba >= threshold).mean(), \n",
        "      **compute_classification_metrics(y_true, proba, threshold)\n",
        "  }\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WtKhHAYXLG6",
        "outputId": "44940168-829f-46b0-adc8-4df8b0045cc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': 0.6957503297069595,\n",
              " 'trigger_rate': 0.4488762007099916,\n",
              " 'accuracy': 0.5494345716449556,\n",
              " 'macro_precision': 0.03751222110651815,\n",
              " 'macro_recall': 0.4515383851132274,\n",
              " 'macro_f1': 0.05788837281625718,\n",
              " 'micro_precision': 0.05042332192753486,\n",
              " 'micro_recall': 0.4820129933094153,\n",
              " 'micro_f1': 0.09129616308789543,\n",
              " 'weighted_precision': 0.053205492346381784,\n",
              " 'weighted_recall': 0.4820129933094153,\n",
              " 'weighted_f1': 0.08328313484048709}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    model, \n",
        "    val_dataset, \n",
        "    batch_size = args.eval_batch_size, \n",
        "    threshold = args.classification_threshold, \n",
        "    device = args.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(\n",
        "    model, \n",
        "    archive_dir, \n",
        "    model_name, \n",
        "    checkpoint_id=\"?\", \n",
        "    metadata=None,\n",
        "    tokenizer=None, \n",
        "    optimizer=None, \n",
        "    scheduler=None,\n",
        "):\n",
        "  # create archive folder\n",
        "  archive_path = os.path.join(archive_dir, model_name)\n",
        "  if not os.path.exists(archive_path):\n",
        "    os.makedirs(archive_path, exist_ok=True)\n",
        "\n",
        "  # create checkpoint folder\n",
        "  checkpoint_dir = os.path.join(archive_path, 'checkpoint-%s' % str(checkpoint_id))\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "  # save model in checkpoint\n",
        "  model_to_save = (model.module if hasattr(model, \"module\") else model)\n",
        "  model_to_save.save_pretrained(checkpoint_dir)\n",
        "  if tokenizer is not None:\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "  if metadata:\n",
        "    torch.save(metadata, os.path.join(checkpoint_dir, \"meta.bin\"))\n",
        "  if scheduler is not None:\n",
        "    torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
        "  if optimizer is not None:\n",
        "    torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
        "\n",
        "  return archive_path\n",
        "\n",
        "def load_from_checkpoint(\n",
        "    archive_dir, \n",
        "    model_name, \n",
        "    checkpoint_id=\"?\", \n",
        "    load_tokenizer=False, \n",
        "    load_metadata=True, \n",
        "    load_optimizer=False, \n",
        "    cls = GoEmotionClassifer\n",
        "):\n",
        "  archive_path = os.path.join(archive_dir, model_name)\n",
        "  checkpoint_dir = os.path.join(archive_path, 'checkpoint-%s' % str(checkpoint_id))\n",
        "\n",
        "  assert os.path.exists(archive_path), archive_path\n",
        "  assert os.path.exists(checkpoint_dir), checkpoint_dir\n",
        "\n",
        "  model = getattr(cls, 'from_pretrained')(\n",
        "      checkpoint_dir\n",
        "  )\n",
        "\n",
        "  output = (model, )\n",
        "  if load_tokenizer:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
        "    output += (tokenizer, )\n",
        "\n",
        "  if load_metadata or load_optimizer:\n",
        "    metadata = torch.load(os.path.join(checkpoint_dir, 'meta.bin'))\n",
        "    if load_metadata:\n",
        "      output += (metadata, )\n",
        "\n",
        "  if load_optimizer:\n",
        "      grouped_parameters = [{'params': [param for name, param in model.named_parameters() \\\n",
        "                                          if not any(nd in name for nd in ('bias', 'LayerNorm.weight'))]}, \n",
        "                            {'params': [param for name, param in model.named_parameters() \\\n",
        "                                        if any(nd in name for nd in ('bias', 'LayerNorm.weight'))]}]\n",
        "\n",
        "      optimizer = AdamW(grouped_parameters, \n",
        "                    lr=metadata['learning_rate'], \n",
        "                    weight_decay=metadata['weight_decay']) \n",
        "      \n",
        "      scheduler = get_linear_schedule_with_warmup(\n",
        "          optimizer,\n",
        "          num_warmup_steps=int(metadata['train_max_step'] * metadata['warmup_ratio']),\n",
        "          num_training_steps=metadata['train_max_step']\n",
        "      )\n",
        "      optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'optimizer.pt')))\n",
        "      scheduler.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'scheduler.pt')))\n",
        "\n",
        "      output += (optimizer, scheduler)\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "1x4ROMXa-TNs"
      },
      "outputs": [],
      "source": [
        "args = args.copy()\n",
        "args['model_name'] = 'go-emotion-tiny-gru'\n",
        "args['seq_len'] = 82\n",
        "args['rnn_hidden'] = 50\n",
        "args['rnn_num_layers'] = 1\n",
        "args['bidirectional'] = True\n",
        "args['hiddens'] = [25],\n",
        "args['validation_steps'] = 1.5e3\n",
        "args['save_steps'] = 1.5e3\n",
        "args['train_batch_size'] = 32\n",
        "args['eval_batch_size'] = 32\n",
        "args['weight_decay'] = 0\n",
        "args = AttrDict(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='prajjwal1/bert-tiny', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, metadata = load_from_checkpoint(\n",
        "    '', \n",
        "    'go-emotion-tiny-gru', \n",
        "    checkpoint_id= '6000', \n",
        "    cls=GoEmotionClassiferWithGRU    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4861164281045397"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata['tr_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proba_to_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('admiration', 'approval', 'love')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "proba_to_emotion(proba_on_examples(['hello I am very confused right now?'], model, tokenizer), 0.15, args.emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\dumps\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': 0.20021999056108034,\n",
              " 'trigger_rate': 0.0,\n",
              " 'accuracy': 0.95304311400693,\n",
              " 'macro_precision': 0.0,\n",
              " 'macro_recall': 0.0,\n",
              " 'macro_f1': 0.0,\n",
              " 'micro_precision': 0.0,\n",
              " 'micro_recall': 0.0,\n",
              " 'micro_f1': 0.0,\n",
              " 'weighted_precision': 0.0,\n",
              " 'weighted_recall': 0.0,\n",
              " 'weighted_f1': 0.0}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    model, \n",
        "    val_dataset, \n",
        "    batch_size = args.eval_batch_size, \n",
        "    threshold = args.classification_threshold, \n",
        "    device = args.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0CKM-CkU4H",
        "outputId": "eacf4978-5e50-460d-cc5a-a04987f9d4ee"
      },
      "outputs": [],
      "source": [
        "# model, metadata = load_from_checkpoint(\n",
        "#     output_dir, \n",
        "#     'go-emotion-tiny', \n",
        "#     checkpoint_id='450000', \n",
        "# )\n",
        "# proba = proba_on_examples(['what the fuck is happening?', \n",
        "#                            'I am not sure', \n",
        "#                            'good job ! cool'], \n",
        "#                   model, tokenizer = tokenizer)\n",
        "\n",
        "# proba_to_emotion(proba, 0.5, args.emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ0XubWhljHR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of bert-tf",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
